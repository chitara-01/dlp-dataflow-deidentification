name: Java CI with Gradle

on:
  push:
    branches:
      - ci-workflow-testing 

env:
  PROJECT_ID: "dlp-dataflow-deid-ci-392604"
  DATASET_ID: "demo_dataset"
  GCS_BUCKET: "dlp-dataflow-deid-ci-392604-demo-data"
  INPUT_FILE_NAME: "tiny_csv"
  INSPECT_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/inspectTemplates/dlp-demo-inspect-latest-1689137435622"
  DEID_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/deidentifyTemplates/dlp-demo-deid-latest-1689137435622"
  REID_TEMPLATE_PATH: "projects/dlp-dataflow-deid-ci-392604/locations/global/deidentifyTemplates/dlp-demo-reid-latest-1689137435622"
  SERVICE_ACCOUNT_EMAIL: "demo-service-account@dlp-dataflow-deid-ci-392604.iam.gserviceaccount.com"
  PUBSUB_TOPIC_NAME: "demo-topic"
  INSPECTION_TABLE_ID: "dlp_inspection_result"
  NUM_INSPECTION_RECORDS: "30"
  REIDENTIFICATION_QUERY_FILE: "reid_query.sql"

jobs:
  inspection:
    runs-on:
      - self-hosted
      - test-runner-chitara01

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Run DLP Pipeline
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --streaming \
                --enableStreamingEngine \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=1 \
                --maxNumWorkers=2 \
                --runner=DataflowRunner \
                --filePattern=gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}.csv \
                --dataset=${{env.DATASET_ID}} \
                --inspectTemplateName=${{env.INSPECT_TEMPLATE_PATH}} \
                --batchSize=200000 \
                --DLPMethod=INSPECT \
                --serviceAccount=$SERVICE_ACCOUNT_EMAIL"

      - name: Verify BQ table
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INSPECTION_TABLE_ID}}"'  | wc -l ) -1))
            if [[ "$table_count" == "1" ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "Verified number of tables in BQ with id ${{env.INSPECTION_TABLE_ID}}: $table_count ."

      - name: Verify distinct rows
        run: |
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(*) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INSPECTION_TABLE_ID}}`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == ${{env.NUM_INSPECTION_RECORDS}} ]]; then
              echo "PASSED";
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "Verified number of rows in ${{env.INSPECTION_TABLE_ID}}: $row_count."

  de-identification:
    runs-on:
      - self-hosted
      - test-runner-chitara01

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Run DLP Pipeline
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=2 \
                --maxNumWorkers=3 \
                --runner=DataflowRunner \
                --filePattern=gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}.csv \
                --dataset=${{env.DATASET_ID}} \
                --inspectTemplateName=${{env.INSPECT_TEMPLATE_PATH}} \
                --deidentifyTemplateName=${{env.DEID_TEMPLATE_PATH}} \
                --batchSize=200000 \
                --DLPMethod=DEID \
                --serviceAccount=${SERVICE_ACCOUNT_EMAIL}"

      - name: Verify BQ tables
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INPUT_FILE_NAME}}"' | wc -l ) -1))
            if [[ "$table_count" == "1" ]]; then
              echo "PASSED"; 
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "Verified number of tables in BQ with id ${{env.INPUT_FILE_NAME}}: $table_count ."

      - name: Verify distinct rows
        run: |
          rc_orig=$(($(gcloud storage cat gs://${{env.GCS_BUCKET}}/${{env.INPUT_FILE_NAME}}.csv | wc -l ) -1))
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(DISTINCT(ID)) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == "$rc_orig" ]]; then 
              echo "PASSED"; 
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "# records in input CSV file are: $rc_orig."
          echo "Verified number of rows in ${{env.INPUT_FILE_NAME}}: $row_count."

  re-identification:
    needs: de-identification

    runs-on:
      - self-hosted
      - test-runner-chitara01

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Setup Java
        uses: actions/setup-java@v1
        with:
          java-version: 11

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2

      - name: Store query in GCS bucket
        run: |
          export QUERY="SELECT ID, Card_Number, Card_Holders_Name FROM \`${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}\`"
          cat << EOF | gsutil cp - gs://${GCS_BUCKET}/${{env.REIDENTIFICATION_QUERY_FILE}}
          ${QUERY}
          EOF

      - name: Create a PubSub topic
        run: |
          if [[ $(gcloud pubsub topics list --filter="name:${{env.PUBSUB_TOPIC_NAME}}") ]]; then
           echo "Topic already created!"
          else
             gcloud pubsub topics create ${{env.PUBSUB_TOPIC_NAME}}
             echo "Created a new topic!"
          fi

      - name: Run DLP Pipeline
        run: |
          gradle run -DmainClass=com.google.swarm.tokenization.DLPTextToBigQueryStreamingV2 -Pargs=" \
                --region=us-central1 \
                --project=${{env.PROJECT_ID}} \
                --tempLocation=gs://${{env.GCS_BUCKET}}/temp \
                --numWorkers=1 \
                --maxNumWorkers=2 \
                --runner=DataflowRunner \
                --tableRef=${{env.PROJECT_ID}}:${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}} \
                --dataset=${{env.DATASET_ID}} \
                --topic=projects/${{env.PROJECT_ID}}/topics/${{env.PUBSUB_TOPIC_NAME}} \
                --autoscalingAlgorithm=THROUGHPUT_BASED \
                --workerMachineType=n1-highmem-4 \
                --deidentifyTemplateName=${{env.REID_TEMPLATE_PATH}} \
                --DLPMethod=REID \
                --keyRange=1024 \
                --queryPath=gs://${GCS_BUCKET}/${{env.REIDENTIFICATION_QUERY_FILE}} \
                --serviceAccount=${SERVICE_ACCOUNT_EMAIL}"

      - name: Verify BQ table
        run: |
          not_verified=true
          table_count=0
          while $not_verified; do
            table_count=$(($(bq query --use_legacy_sql=false --format csv 'SELECT * FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}`.__TABLES__ WHERE table_id="${{env.INPUT_FILE_NAME}}_re_id"'  | wc -l ) -1))
            if [[ "$table_count" == "1" ]]; then
              echo "PASSED"; 
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "Verified number of tables in BQ with id ${{env.INPUT_FILE_NAME}}_re_id: $table_count ."

      - name: Verify distinct rows
        run: |
          rc_orig=$(($(bq query --nouse_legacy_sql --format=csv "$(gcloud storage cat gs://${{env.GCS_BUCKET}}/${{env.REIDENTIFICATION_QUERY_FILE}})" | wc -l) - 1))
          not_verified=true
          row_count=0
          while $not_verified; do
            row_count_json=$(bq query --use_legacy_sql=false --format json 'SELECT COUNT(ID) FROM `${{env.PROJECT_ID}}.${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}_re_id`')
            row_count=$(echo "$row_count_json" | jq -r '.[].f0_')
            if [[ "$row_count" == "$rc_orig" ]]; then 
              echo "PASSED"; 
              not_verified=false;
            else
              sleep 30s
            fi
          done
          echo "# records in input query are: $rc_orig."
          echo "Verified number of rows in ${{env.INPUT_FILE_NAME}}_re_id: $row_count."

  clean-up:
    if: ${{ always() }}
    needs:
      - inspection
      - de-identification
      - re-identification

    runs-on:
      - self-hosted
      - test-runner-chitara01

    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v2

      - name: Clean-up BQ dataset
        run: |
          bq rm -t -f ${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}
          bq rm -t -f ${{env.DATASET_ID}}.${{env.INPUT_FILE_NAME}}_re_id
          bq rm -t -f ${{env.DATASET_ID}}.${{env.INSPECTION_TABLE_ID}}
